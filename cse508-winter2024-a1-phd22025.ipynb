{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7574631,"sourceType":"datasetVersion","datasetId":4410177}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nimport numpy as np\nimport pandas as pd\nimport random\nimport pickle\nimport re\nnltk.download('stopwords')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-09T17:45:08.827784Z","iopub.execute_input":"2024-02-09T17:45:08.828843Z","iopub.status.idle":"2024-02-09T17:45:10.584808Z","shell.execute_reply.started":"2024-02-09T17:45:08.828784Z","shell.execute_reply":"2024-02-09T17:45:10.583501Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# Creating a dataframe from the text files ","metadata":{}},{"cell_type":"code","source":"# Making a column of the .txt files path from the directory\nimport glob\ndirectory = '/kaggle/input/dataset-ir-assignment1/text_files/'\ntxt_files = glob.glob(directory + '*.txt')\nprint(len(txt_files))\ndf = pd.DataFrame(columns = [\"path\"])\ndf[\"path\"] = txt_files\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:10.591213Z","iopub.execute_input":"2024-02-09T17:45:10.591648Z","iopub.status.idle":"2024-02-09T17:45:10.619952Z","shell.execute_reply.started":"2024-02-09T17:45:10.591603Z","shell.execute_reply":"2024-02-09T17:45:10.618824Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"999\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                path\n0  /kaggle/input/dataset-ir-assignment1/text_file...\n1  /kaggle/input/dataset-ir-assignment1/text_file...\n2  /kaggle/input/dataset-ir-assignment1/text_file...\n3  /kaggle/input/dataset-ir-assignment1/text_file...\n4  /kaggle/input/dataset-ir-assignment1/text_file...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/dataset-ir-assignment1/text_file...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/dataset-ir-assignment1/text_file...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/dataset-ir-assignment1/text_file...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/dataset-ir-assignment1/text_file...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/dataset-ir-assignment1/text_file...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Getting the contents of the .txt files from the directory to a parallel column in the previously created dataframe for easier data preprocessing\ncontent_list = []\nfor index, row in df.iterrows():\n    file_path = row['path']\n    try:\n        with open(file_path, 'r') as file:\n            content_list.append(file.read())\n    except FileNotFoundError:\n        print(f\"File '{file_path}' not found.\")\n\nprint(len(content_list))\n\n\ndf['text'] = content_list\nprint(df['text'].shape)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:10.621667Z","iopub.execute_input":"2024-02-09T17:45:10.622844Z","iopub.status.idle":"2024-02-09T17:45:12.174708Z","shell.execute_reply.started":"2024-02-09T17:45:10.622811Z","shell.execute_reply":"2024-02-09T17:45:12.173287Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"999\n(999,)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                path  \\\n0  /kaggle/input/dataset-ir-assignment1/text_file...   \n1  /kaggle/input/dataset-ir-assignment1/text_file...   \n2  /kaggle/input/dataset-ir-assignment1/text_file...   \n3  /kaggle/input/dataset-ir-assignment1/text_file...   \n4  /kaggle/input/dataset-ir-assignment1/text_file...   \n\n                                                text  \n0  The Amazon advertised pictures of this item is...  \n1  I really like the simplicity of this bridge. I...  \n2  Truthfully, I had no idea that the even were u...  \n3  My ES*335 fit loose and needed some padding ad...  \n4  I bought a used MIM strat that came with a bla...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/dataset-ir-assignment1/text_file...</td>\n      <td>The Amazon advertised pictures of this item is...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/dataset-ir-assignment1/text_file...</td>\n      <td>I really like the simplicity of this bridge. I...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/dataset-ir-assignment1/text_file...</td>\n      <td>Truthfully, I had no idea that the even were u...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/dataset-ir-assignment1/text_file...</td>\n      <td>My ES*335 fit loose and needed some padding ad...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/dataset-ir-assignment1/text_file...</td>\n      <td>I bought a used MIM strat that came with a bla...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.to_csv(\"/kaggle/working/data.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:12.179302Z","iopub.execute_input":"2024-02-09T17:45:12.179725Z","iopub.status.idle":"2024-02-09T17:45:12.215130Z","shell.execute_reply.started":"2024-02-09T17:45:12.179694Z","shell.execute_reply":"2024-02-09T17:45:12.213978Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/data.csv\")\ndf = df.dropna().reset_index(drop =True)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:12.217873Z","iopub.execute_input":"2024-02-09T17:45:12.218692Z","iopub.status.idle":"2024-02-09T17:45:12.240712Z","shell.execute_reply.started":"2024-02-09T17:45:12.218653Z","shell.execute_reply":"2024-02-09T17:45:12.239446Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Q1. Data Preprocessing ","metadata":{}},{"cell_type":"markdown","source":"1. Perform the following preprocessing steps on each of the text files in the dataset linked\nabove.\n\na. Lowercase the text\n\n\nb. Perform tokenization\n\n\nc. Remove stopwords\n\n\nd. Remove punctuations\n\n\ne. Remove blank space tokens","metadata":{}},{"cell_type":"code","source":"def cleaning(data):\n    corpus = []\n    for i in range(0, len(data)):\n        sentence = re.sub('[^a-zA-Z]', ' ', str(data[i])) # Removing Punctuation Marks\n        sentence = sentence.lower() # Lowering the text\n        sentence = sentence.split() # Tokenization\n        \n        # Remove blank space tokens\n        sentence = [word for word in sentence if word.strip()]  # Removes empty strings\n        \n        all_stopwords = stopwords.words('english') # Removing the stopwords\n        #all_stopwords.remove('not')\n        \n        sentence = [word for word in sentence if not word in set(all_stopwords)]\n        sentence = ' '.join(sentence)\n        corpus.append(sentence)\n      \n    return corpus","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:12.242115Z","iopub.execute_input":"2024-02-09T17:45:12.242642Z","iopub.status.idle":"2024-02-09T17:45:12.251264Z","shell.execute_reply.started":"2024-02-09T17:45:12.242610Z","shell.execute_reply":"2024-02-09T17:45:12.250204Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df['preprocessed_text'] = cleaning(df['text'])\ndf.head()\ndf.to_csv(\"/kaggle/working/preprocessed_dataset.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:12.252912Z","iopub.execute_input":"2024-02-09T17:45:12.253634Z","iopub.status.idle":"2024-02-09T17:45:12.915869Z","shell.execute_reply.started":"2024-02-09T17:45:12.253595Z","shell.execute_reply":"2024-02-09T17:45:12.914943Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Sample of five examples: original text vs pre-processed text","metadata":{}},{"cell_type":"code","source":"def sample_cleaning(data):\n    corpus = []\n    for i in range(0, min(len(data), 5)):\n        original_text = data[i]\n        print(\"Original Text:\")\n        print(original_text)\n        \n        # Preprocessing steps\n        sentence = re.sub('[^a-zA-Z]', ' ', str(data[i])) # Removing Punctuation Marks\n        print(\"\\nAfter Removing Punctuation Marks:\")\n        print(sentence)\n        \n        sentence = sentence.lower() # Lowering the text\n        print(\"\\nAfter Lowercasing:\")\n        print(sentence)\n        \n        # Tokenization\n        sentence = sentence.split()\n        print(\"\\nAfter Tokenization:\")\n        print(sentence)\n        \n        # Remove blank space tokens\n        sentence = [word for word in sentence if word.strip()]  \n        print(\"\\nAfter Removing Blank Space Tokens:\")\n        print(sentence)\n        \n        all_stopwords = stopwords.words('english') # Removing the stopwords\n        sentence = [word for word in sentence if not word in set(all_stopwords)]\n        sentence = ' '.join(sentence)\n        \n        # Print preprocessed text\n        print(\"\\nAfter Removing Stopwords:\")\n        print(sentence)\n        print(\"\\n\")\n        \n        corpus.append(sentence)\n      \n    return corpus\n\ncleaned_data = sample_cleaning(df['text'][:5])","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:12.917556Z","iopub.execute_input":"2024-02-09T17:45:12.918163Z","iopub.status.idle":"2024-02-09T17:45:12.934845Z","shell.execute_reply.started":"2024-02-09T17:45:12.918131Z","shell.execute_reply":"2024-02-09T17:45:12.933408Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Original Text:\nThe Amazon advertised pictures of this item is a Fender FTE3 pre amp. The description says it's a Fishman Isys III type. I googled it and found out there's two version of pre amp type of this T Bucket 300 CE series. The one I got today is a Fishman type pre amp which is for me much better that FTE3. Out of the box, I fell inlove with the color and design. Mine is a 3 tone sunburst. The action is just right for me. The tone/sound is awesome. I hooked it up in my Vox amp and I have to say this thing really rocks. Of course you can get a better sounding acoustic/electric guitar out there for a very much expensive price. I'm a family man with two kids and I am not gonna spend a thousand bucks just for a hobby. Amplified or not I love the tone of this baby. For it's appearance, sound quality, and of course THE PRICE, I'm giving it a five stars.\n\nI uploaded some pictures (above) of the Fishman pre amp version of this T bucket series. :O)..\n\nAfter Removing Punctuation Marks:\nThe Amazon advertised pictures of this item is a Fender FTE  pre amp  The description says it s a Fishman Isys III type  I googled it and found out there s two version of pre amp type of this T Bucket     CE series  The one I got today is a Fishman type pre amp which is for me much better that FTE   Out of the box  I fell inlove with the color and design  Mine is a   tone sunburst  The action is just right for me  The tone sound is awesome  I hooked it up in my Vox amp and I have to say this thing really rocks  Of course you can get a better sounding acoustic electric guitar out there for a very much expensive price  I m a family man with two kids and I am not gonna spend a thousand bucks just for a hobby  Amplified or not I love the tone of this baby  For it s appearance  sound quality  and of course THE PRICE  I m giving it a five stars   I uploaded some pictures  above  of the Fishman pre amp version of this T bucket series   O   \n\nAfter Lowercasing:\nthe amazon advertised pictures of this item is a fender fte  pre amp  the description says it s a fishman isys iii type  i googled it and found out there s two version of pre amp type of this t bucket     ce series  the one i got today is a fishman type pre amp which is for me much better that fte   out of the box  i fell inlove with the color and design  mine is a   tone sunburst  the action is just right for me  the tone sound is awesome  i hooked it up in my vox amp and i have to say this thing really rocks  of course you can get a better sounding acoustic electric guitar out there for a very much expensive price  i m a family man with two kids and i am not gonna spend a thousand bucks just for a hobby  amplified or not i love the tone of this baby  for it s appearance  sound quality  and of course the price  i m giving it a five stars   i uploaded some pictures  above  of the fishman pre amp version of this t bucket series   o   \n\nAfter Tokenization:\n['the', 'amazon', 'advertised', 'pictures', 'of', 'this', 'item', 'is', 'a', 'fender', 'fte', 'pre', 'amp', 'the', 'description', 'says', 'it', 's', 'a', 'fishman', 'isys', 'iii', 'type', 'i', 'googled', 'it', 'and', 'found', 'out', 'there', 's', 'two', 'version', 'of', 'pre', 'amp', 'type', 'of', 'this', 't', 'bucket', 'ce', 'series', 'the', 'one', 'i', 'got', 'today', 'is', 'a', 'fishman', 'type', 'pre', 'amp', 'which', 'is', 'for', 'me', 'much', 'better', 'that', 'fte', 'out', 'of', 'the', 'box', 'i', 'fell', 'inlove', 'with', 'the', 'color', 'and', 'design', 'mine', 'is', 'a', 'tone', 'sunburst', 'the', 'action', 'is', 'just', 'right', 'for', 'me', 'the', 'tone', 'sound', 'is', 'awesome', 'i', 'hooked', 'it', 'up', 'in', 'my', 'vox', 'amp', 'and', 'i', 'have', 'to', 'say', 'this', 'thing', 'really', 'rocks', 'of', 'course', 'you', 'can', 'get', 'a', 'better', 'sounding', 'acoustic', 'electric', 'guitar', 'out', 'there', 'for', 'a', 'very', 'much', 'expensive', 'price', 'i', 'm', 'a', 'family', 'man', 'with', 'two', 'kids', 'and', 'i', 'am', 'not', 'gonna', 'spend', 'a', 'thousand', 'bucks', 'just', 'for', 'a', 'hobby', 'amplified', 'or', 'not', 'i', 'love', 'the', 'tone', 'of', 'this', 'baby', 'for', 'it', 's', 'appearance', 'sound', 'quality', 'and', 'of', 'course', 'the', 'price', 'i', 'm', 'giving', 'it', 'a', 'five', 'stars', 'i', 'uploaded', 'some', 'pictures', 'above', 'of', 'the', 'fishman', 'pre', 'amp', 'version', 'of', 'this', 't', 'bucket', 'series', 'o']\n\nAfter Removing Blank Space Tokens:\n['the', 'amazon', 'advertised', 'pictures', 'of', 'this', 'item', 'is', 'a', 'fender', 'fte', 'pre', 'amp', 'the', 'description', 'says', 'it', 's', 'a', 'fishman', 'isys', 'iii', 'type', 'i', 'googled', 'it', 'and', 'found', 'out', 'there', 's', 'two', 'version', 'of', 'pre', 'amp', 'type', 'of', 'this', 't', 'bucket', 'ce', 'series', 'the', 'one', 'i', 'got', 'today', 'is', 'a', 'fishman', 'type', 'pre', 'amp', 'which', 'is', 'for', 'me', 'much', 'better', 'that', 'fte', 'out', 'of', 'the', 'box', 'i', 'fell', 'inlove', 'with', 'the', 'color', 'and', 'design', 'mine', 'is', 'a', 'tone', 'sunburst', 'the', 'action', 'is', 'just', 'right', 'for', 'me', 'the', 'tone', 'sound', 'is', 'awesome', 'i', 'hooked', 'it', 'up', 'in', 'my', 'vox', 'amp', 'and', 'i', 'have', 'to', 'say', 'this', 'thing', 'really', 'rocks', 'of', 'course', 'you', 'can', 'get', 'a', 'better', 'sounding', 'acoustic', 'electric', 'guitar', 'out', 'there', 'for', 'a', 'very', 'much', 'expensive', 'price', 'i', 'm', 'a', 'family', 'man', 'with', 'two', 'kids', 'and', 'i', 'am', 'not', 'gonna', 'spend', 'a', 'thousand', 'bucks', 'just', 'for', 'a', 'hobby', 'amplified', 'or', 'not', 'i', 'love', 'the', 'tone', 'of', 'this', 'baby', 'for', 'it', 's', 'appearance', 'sound', 'quality', 'and', 'of', 'course', 'the', 'price', 'i', 'm', 'giving', 'it', 'a', 'five', 'stars', 'i', 'uploaded', 'some', 'pictures', 'above', 'of', 'the', 'fishman', 'pre', 'amp', 'version', 'of', 'this', 't', 'bucket', 'series', 'o']\n\nAfter Removing Stopwords:\namazon advertised pictures item fender fte pre amp description says fishman isys iii type googled found two version pre amp type bucket ce series one got today fishman type pre amp much better fte box fell inlove color design mine tone sunburst action right tone sound awesome hooked vox amp say thing really rocks course get better sounding acoustic electric guitar much expensive price family man two kids gonna spend thousand bucks hobby amplified love tone baby appearance sound quality course price giving five stars uploaded pictures fishman pre amp version bucket series\n\n\nOriginal Text:\nI really like the simplicity of this bridge. It adjusts easy for string height and length. It comes with all the mounting screws and even a small allen wrench to make the height adjustments. The provided mounting screws are Phillips head screws. The one thing that I missed when I ordered this was the fact that it is not a bridge for string-thru the guitar body placement. The strings dead end to the stop-flair of the plate. I included a picture of this so it may help to make others aware that it is not for string-thru applications. No biggie tho for me, I will use it on another project now.\n\nAfter Removing Punctuation Marks:\nI really like the simplicity of this bridge  It adjusts easy for string height and length  It comes with all the mounting screws and even a small allen wrench to make the height adjustments  The provided mounting screws are Phillips head screws  The one thing that I missed when I ordered this was the fact that it is not a bridge for string thru the guitar body placement  The strings dead end to the stop flair of the plate  I included a picture of this so it may help to make others aware that it is not for string thru applications  No biggie tho for me  I will use it on another project now \n\nAfter Lowercasing:\ni really like the simplicity of this bridge  it adjusts easy for string height and length  it comes with all the mounting screws and even a small allen wrench to make the height adjustments  the provided mounting screws are phillips head screws  the one thing that i missed when i ordered this was the fact that it is not a bridge for string thru the guitar body placement  the strings dead end to the stop flair of the plate  i included a picture of this so it may help to make others aware that it is not for string thru applications  no biggie tho for me  i will use it on another project now \n\nAfter Tokenization:\n['i', 'really', 'like', 'the', 'simplicity', 'of', 'this', 'bridge', 'it', 'adjusts', 'easy', 'for', 'string', 'height', 'and', 'length', 'it', 'comes', 'with', 'all', 'the', 'mounting', 'screws', 'and', 'even', 'a', 'small', 'allen', 'wrench', 'to', 'make', 'the', 'height', 'adjustments', 'the', 'provided', 'mounting', 'screws', 'are', 'phillips', 'head', 'screws', 'the', 'one', 'thing', 'that', 'i', 'missed', 'when', 'i', 'ordered', 'this', 'was', 'the', 'fact', 'that', 'it', 'is', 'not', 'a', 'bridge', 'for', 'string', 'thru', 'the', 'guitar', 'body', 'placement', 'the', 'strings', 'dead', 'end', 'to', 'the', 'stop', 'flair', 'of', 'the', 'plate', 'i', 'included', 'a', 'picture', 'of', 'this', 'so', 'it', 'may', 'help', 'to', 'make', 'others', 'aware', 'that', 'it', 'is', 'not', 'for', 'string', 'thru', 'applications', 'no', 'biggie', 'tho', 'for', 'me', 'i', 'will', 'use', 'it', 'on', 'another', 'project', 'now']\n\nAfter Removing Blank Space Tokens:\n['i', 'really', 'like', 'the', 'simplicity', 'of', 'this', 'bridge', 'it', 'adjusts', 'easy', 'for', 'string', 'height', 'and', 'length', 'it', 'comes', 'with', 'all', 'the', 'mounting', 'screws', 'and', 'even', 'a', 'small', 'allen', 'wrench', 'to', 'make', 'the', 'height', 'adjustments', 'the', 'provided', 'mounting', 'screws', 'are', 'phillips', 'head', 'screws', 'the', 'one', 'thing', 'that', 'i', 'missed', 'when', 'i', 'ordered', 'this', 'was', 'the', 'fact', 'that', 'it', 'is', 'not', 'a', 'bridge', 'for', 'string', 'thru', 'the', 'guitar', 'body', 'placement', 'the', 'strings', 'dead', 'end', 'to', 'the', 'stop', 'flair', 'of', 'the', 'plate', 'i', 'included', 'a', 'picture', 'of', 'this', 'so', 'it', 'may', 'help', 'to', 'make', 'others', 'aware', 'that', 'it', 'is', 'not', 'for', 'string', 'thru', 'applications', 'no', 'biggie', 'tho', 'for', 'me', 'i', 'will', 'use', 'it', 'on', 'another', 'project', 'now']\n\nAfter Removing Stopwords:\nreally like simplicity bridge adjusts easy string height length comes mounting screws even small allen wrench make height adjustments provided mounting screws phillips head screws one thing missed ordered fact bridge string thru guitar body placement strings dead end stop flair plate included picture may help make others aware string thru applications biggie tho use another project\n\n\nOriginal Text:\nTruthfully, I had no idea that the even were ukulele capos. But why not? I use them on guitars and banjo, and there are certainly times when you might want to shift key without transposing so you can take advantage of open strings.\n\nRegardless, this is a well made, lightweight capo. Weight is of particular importance when dealing with an instrument that's already very small and lightweight, and hernally played without a strap. This capo won't make your soprano uke neck heavy. The soft rubber surface on the moveable jaw does a good job of pressing synthetic strings against a fret without stretching them out of tune. A useful tool for ukuleleists.\n\nAfter Removing Punctuation Marks:\nTruthfully  I had no idea that the even were ukulele capos  But why not  I use them on guitars and banjo  and there are certainly times when you might want to shift key without transposing so you can take advantage of open strings   Regardless  this is a well made  lightweight capo  Weight is of particular importance when dealing with an instrument that s already very small and lightweight  and hernally played without a strap  This capo won t make your soprano uke neck heavy  The soft rubber surface on the moveable jaw does a good job of pressing synthetic strings against a fret without stretching them out of tune  A useful tool for ukuleleists \n\nAfter Lowercasing:\ntruthfully  i had no idea that the even were ukulele capos  but why not  i use them on guitars and banjo  and there are certainly times when you might want to shift key without transposing so you can take advantage of open strings   regardless  this is a well made  lightweight capo  weight is of particular importance when dealing with an instrument that s already very small and lightweight  and hernally played without a strap  this capo won t make your soprano uke neck heavy  the soft rubber surface on the moveable jaw does a good job of pressing synthetic strings against a fret without stretching them out of tune  a useful tool for ukuleleists \n\nAfter Tokenization:\n['truthfully', 'i', 'had', 'no', 'idea', 'that', 'the', 'even', 'were', 'ukulele', 'capos', 'but', 'why', 'not', 'i', 'use', 'them', 'on', 'guitars', 'and', 'banjo', 'and', 'there', 'are', 'certainly', 'times', 'when', 'you', 'might', 'want', 'to', 'shift', 'key', 'without', 'transposing', 'so', 'you', 'can', 'take', 'advantage', 'of', 'open', 'strings', 'regardless', 'this', 'is', 'a', 'well', 'made', 'lightweight', 'capo', 'weight', 'is', 'of', 'particular', 'importance', 'when', 'dealing', 'with', 'an', 'instrument', 'that', 's', 'already', 'very', 'small', 'and', 'lightweight', 'and', 'hernally', 'played', 'without', 'a', 'strap', 'this', 'capo', 'won', 't', 'make', 'your', 'soprano', 'uke', 'neck', 'heavy', 'the', 'soft', 'rubber', 'surface', 'on', 'the', 'moveable', 'jaw', 'does', 'a', 'good', 'job', 'of', 'pressing', 'synthetic', 'strings', 'against', 'a', 'fret', 'without', 'stretching', 'them', 'out', 'of', 'tune', 'a', 'useful', 'tool', 'for', 'ukuleleists']\n\nAfter Removing Blank Space Tokens:\n['truthfully', 'i', 'had', 'no', 'idea', 'that', 'the', 'even', 'were', 'ukulele', 'capos', 'but', 'why', 'not', 'i', 'use', 'them', 'on', 'guitars', 'and', 'banjo', 'and', 'there', 'are', 'certainly', 'times', 'when', 'you', 'might', 'want', 'to', 'shift', 'key', 'without', 'transposing', 'so', 'you', 'can', 'take', 'advantage', 'of', 'open', 'strings', 'regardless', 'this', 'is', 'a', 'well', 'made', 'lightweight', 'capo', 'weight', 'is', 'of', 'particular', 'importance', 'when', 'dealing', 'with', 'an', 'instrument', 'that', 's', 'already', 'very', 'small', 'and', 'lightweight', 'and', 'hernally', 'played', 'without', 'a', 'strap', 'this', 'capo', 'won', 't', 'make', 'your', 'soprano', 'uke', 'neck', 'heavy', 'the', 'soft', 'rubber', 'surface', 'on', 'the', 'moveable', 'jaw', 'does', 'a', 'good', 'job', 'of', 'pressing', 'synthetic', 'strings', 'against', 'a', 'fret', 'without', 'stretching', 'them', 'out', 'of', 'tune', 'a', 'useful', 'tool', 'for', 'ukuleleists']\n\nAfter Removing Stopwords:\ntruthfully idea even ukulele capos use guitars banjo certainly times might want shift key without transposing take advantage open strings regardless well made lightweight capo weight particular importance dealing instrument already small lightweight hernally played without strap capo make soprano uke neck heavy soft rubber surface moveable jaw good job pressing synthetic strings fret without stretching tune useful tool ukuleleists\n\n\nOriginal Text:\nMy ES*335 fit loose and needed some padding added to keep the guitar snug, but if you don't mind doing that it's a good deal...\n\nAfter Removing Punctuation Marks:\nMy ES     fit loose and needed some padding added to keep the guitar snug  but if you don t mind doing that it s a good deal   \n\nAfter Lowercasing:\nmy es     fit loose and needed some padding added to keep the guitar snug  but if you don t mind doing that it s a good deal   \n\nAfter Tokenization:\n['my', 'es', 'fit', 'loose', 'and', 'needed', 'some', 'padding', 'added', 'to', 'keep', 'the', 'guitar', 'snug', 'but', 'if', 'you', 'don', 't', 'mind', 'doing', 'that', 'it', 's', 'a', 'good', 'deal']\n\nAfter Removing Blank Space Tokens:\n['my', 'es', 'fit', 'loose', 'and', 'needed', 'some', 'padding', 'added', 'to', 'keep', 'the', 'guitar', 'snug', 'but', 'if', 'you', 'don', 't', 'mind', 'doing', 'that', 'it', 's', 'a', 'good', 'deal']\n\nAfter Removing Stopwords:\nes fit loose needed padding added keep guitar snug mind good deal\n\n\nOriginal Text:\nI bought a used MIM strat that came with a black pick guard and creamish colored pickups. The guitar was great but the pick guard wasn't my style. I put this on and now I love the look of this guitar. Highly recommend.\n\nAfter Removing Punctuation Marks:\nI bought a used MIM strat that came with a black pick guard and creamish colored pickups  The guitar was great but the pick guard wasn t my style  I put this on and now I love the look of this guitar  Highly recommend \n\nAfter Lowercasing:\ni bought a used mim strat that came with a black pick guard and creamish colored pickups  the guitar was great but the pick guard wasn t my style  i put this on and now i love the look of this guitar  highly recommend \n\nAfter Tokenization:\n['i', 'bought', 'a', 'used', 'mim', 'strat', 'that', 'came', 'with', 'a', 'black', 'pick', 'guard', 'and', 'creamish', 'colored', 'pickups', 'the', 'guitar', 'was', 'great', 'but', 'the', 'pick', 'guard', 'wasn', 't', 'my', 'style', 'i', 'put', 'this', 'on', 'and', 'now', 'i', 'love', 'the', 'look', 'of', 'this', 'guitar', 'highly', 'recommend']\n\nAfter Removing Blank Space Tokens:\n['i', 'bought', 'a', 'used', 'mim', 'strat', 'that', 'came', 'with', 'a', 'black', 'pick', 'guard', 'and', 'creamish', 'colored', 'pickups', 'the', 'guitar', 'was', 'great', 'but', 'the', 'pick', 'guard', 'wasn', 't', 'my', 'style', 'i', 'put', 'this', 'on', 'and', 'now', 'i', 'love', 'the', 'look', 'of', 'this', 'guitar', 'highly', 'recommend']\n\nAfter Removing Stopwords:\nbought used mim strat came black pick guard creamish colored pickups guitar great pick guard style put love look guitar highly recommend\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q2. Unigram Inverted Index and Boolean Queries","metadata":{}},{"cell_type":"markdown","source":"## 1. Create a unigram inverted index (from scratch; No library allowed) of the dataset obtained from Q1 (after preprocessing).","metadata":{}},{"cell_type":"code","source":"words = df['preprocessed_text'].str.split()\nall_words = [word for sublist in words for word in sublist]\nprint(\"Five Unique Words:\", all_words[0:5])\nunique_words = set(all_words)\nnum_unique_words = len(unique_words)\nprint(\"Number of unique words:\", num_unique_words)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:12.936676Z","iopub.execute_input":"2024-02-09T17:45:12.937201Z","iopub.status.idle":"2024-02-09T17:45:12.960864Z","shell.execute_reply.started":"2024-02-09T17:45:12.937149Z","shell.execute_reply":"2024-02-09T17:45:12.959288Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Five Unique Words: ['amazon', 'advertised', 'pictures', 'item', 'fender']\nNumber of unique words: 6009\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_inverted_index(corpus):\n    inverted_index = {}\n    for doc_id, document in enumerate(corpus):\n        terms = document.split()\n        for term in terms:\n            if term not in inverted_index:\n                inverted_index[term] = set()\n            inverted_index[term].add(doc_id)\n    return inverted_index\n\n\ninverted_index = create_inverted_index(df['preprocessed_text'])\nprint(len(inverted_index))\nprint()\nprint()\nprint(\"Inverted Index for the word:\")\nprint(\"'amazon':\", inverted_index['amazon'])","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:12.962760Z","iopub.execute_input":"2024-02-09T17:45:12.963726Z","iopub.status.idle":"2024-02-09T17:45:13.007790Z","shell.execute_reply.started":"2024-02-09T17:45:12.963675Z","shell.execute_reply":"2024-02-09T17:45:13.006712Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"6009\n\n\nInverted Index for the word:\n'amazon': {0, 769, 130, 386, 773, 7, 9, 270, 654, 144, 401, 530, 655, 534, 151, 920, 538, 157, 418, 420, 549, 295, 807, 170, 555, 300, 557, 693, 952, 954, 445, 957, 191, 64, 450, 70, 326, 327, 456, 330, 838, 466, 83, 595, 346, 476, 350, 354, 614, 616, 745, 878, 880, 881, 371, 764}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Use Pythonâ€™s pickle module to save and load the unigram inverted index.","metadata":{}},{"cell_type":"code","source":"import pickle\n\nwith open('inverted_index.pickle', 'wb') as f:\n    pickle.dump(inverted_index, f)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.009146Z","iopub.execute_input":"2024-02-09T17:45:13.010451Z","iopub.status.idle":"2024-02-09T17:45:13.024845Z","shell.execute_reply.started":"2024-02-09T17:45:13.010387Z","shell.execute_reply":"2024-02-09T17:45:13.023309Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('inverted_index.pickle', 'rb') as f:\n    inverted_index = pickle.load(f)\n\nprint(\"Inverted Index for the word: 'amazon'\", inverted_index['amazon'])","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.026539Z","iopub.execute_input":"2024-02-09T17:45:13.026989Z","iopub.status.idle":"2024-02-09T17:45:13.051131Z","shell.execute_reply.started":"2024-02-09T17:45:13.026950Z","shell.execute_reply":"2024-02-09T17:45:13.049607Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Inverted Index for the word: 'amazon' {0, 769, 130, 386, 773, 7, 9, 270, 654, 144, 401, 530, 655, 534, 151, 920, 538, 157, 418, 420, 549, 295, 807, 170, 555, 300, 557, 693, 952, 954, 445, 957, 191, 64, 450, 70, 326, 327, 456, 330, 838, 466, 83, 595, 346, 476, 350, 354, 614, 616, 745, 878, 880, 881, 371, 764}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Provide support for the following operations:\na. T1 AND T2\n\n\nb. T1 OR T2\n\n\nc. T1 AND NOT T2\n\n\nd. T1 OR NOT T2","metadata":{}},{"cell_type":"code","source":"def create_inverted_index(corpus):\n    inverted_index = {}\n    doc_term_freq = {}  # Dictionary to store term frequencies per document\n    for doc_id, document in enumerate(corpus):\n        terms = document.split()\n        doc_term_freq[doc_id] = {}\n        for term in terms:\n            if term not in inverted_index:\n                inverted_index[term] = set()\n            inverted_index[term].add(doc_id)\n            doc_term_freq[doc_id][term] = doc_term_freq[doc_id].get(term, 0) + 1\n    return inverted_index, doc_term_freq\n\ndef boolean_AND(inverted_index, T1, T2):\n    if T1 in inverted_index and T2 in inverted_index:\n        return inverted_index[T1].intersection(inverted_index[T2])\n    else:\n        return set()\n\ndef boolean_OR(inverted_index, T1, T2):\n    if T1 in inverted_index and T2 in inverted_index:\n        return inverted_index[T1].union(inverted_index[T2])\n    elif T1 in inverted_index:\n        return inverted_index[T1]\n    elif T2 in inverted_index:\n        return inverted_index[T2]\n    else:\n        return set()\n\ndef boolean_AND_NOT(inverted_index, doc_term_freq, T1, T2):\n    result = set(inverted_index.get(T1, set()))\n    if T2 in inverted_index:\n        result.difference_update(inverted_index[T2])\n    return result\n\ndef boolean_OR_NOT(inverted_index, doc_term_freq, T1, T2):\n    result = set(inverted_index.get(T1, set()))\n    if T2 in inverted_index:\n        result.difference_update(inverted_index[T2])\n    return result\n\n\ninverted_index, doc_term_freq = create_inverted_index(df['preprocessed_text'])\nresult = boolean_AND(inverted_index, 'amazon', 'advertised')\n#result = boolean_OR(inverted_index, 'T1', 'T2')\n#result = boolean_AND_NOT(inverted_index, doc_term_freq, 'T1', 'T2')\n#result = boolean_OR_NOT(inverted_index, doc_term_freq, 'T1', 'T2')\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.057063Z","iopub.execute_input":"2024-02-09T17:45:13.057452Z","iopub.status.idle":"2024-02-09T17:45:13.123279Z","shell.execute_reply.started":"2024-02-09T17:45:13.057422Z","shell.execute_reply":"2024-02-09T17:45:13.121808Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{0, 807}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(inverted_index[\"amazon\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.125237Z","iopub.execute_input":"2024-02-09T17:45:13.125773Z","iopub.status.idle":"2024-02-09T17:45:13.133239Z","shell.execute_reply.started":"2024-02-09T17:45:13.125733Z","shell.execute_reply":"2024-02-09T17:45:13.131765Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"{0, 769, 130, 386, 773, 7, 9, 270, 654, 144, 401, 530, 655, 534, 151, 920, 538, 157, 418, 420, 549, 295, 807, 170, 555, 300, 557, 693, 952, 954, 445, 957, 191, 64, 450, 70, 326, 327, 456, 330, 838, 466, 83, 595, 346, 476, 350, 354, 614, 616, 745, 878, 880, 881, 371, 764}\n","output_type":"stream"}]},{"cell_type":"code","source":"def process_query(corpus, query):\n    # Create inverted index and term frequency dictionary\n    inverted_index = {}\n    doc_term_freq = {}\n    for doc_id, document in enumerate(corpus):\n        terms = document.split()\n        doc_term_freq[doc_id] = {}\n        for term in terms:\n            if term not in inverted_index:\n                inverted_index[term] = set()\n            inverted_index[term].add(doc_id)\n            doc_term_freq[doc_id][term] = doc_term_freq[doc_id].get(term, 0) + 1\n    \n    # Defining boolean operations\n    def boolean_AND(operands):\n        result = set(range(len(corpus)))\n        for operand in operands:\n            result = result.intersection(operand)\n        return result\n\n    def boolean_OR(operands):\n        result = set()\n        for operand in operands:\n            result = result.union(operand)\n        return result\n\n    def boolean_AND_NOT(operands):\n        result = operands[0]\n        for operand in operands[1:]:\n            result = result.difference(operand)\n        return result\n\n    def boolean_OR_NOT(operands):\n        result = operands[0]\n        for operand in operands[1:]:\n            result = result.union(result.difference(operand))\n        return result\n\n    # Processing query\n    stack = []\n    operators = set(['AND', 'OR', 'NOT'])\n    query_terms = query.split()\n    for term in query_terms:\n        if term not in operators:\n            stack.append(inverted_index.get(term, set()))\n        elif term == 'NOT':\n            if stack:\n                operand = stack.pop()\n                result = set(range(len(corpus))).difference(operand)\n                stack.append(result)\n        else:\n            operands = []\n            while stack and stack[-1] not in operators:\n                operands.append(stack.pop())\n            if operands:\n                if term == 'AND':\n                    stack.append(boolean_AND(operands))\n                elif term == 'OR':\n                    stack.append(boolean_OR(operands))\n                elif term == 'AND_NOT':\n                    stack.append(boolean_AND_NOT(operands))\n                elif term == 'OR_NOT':\n                    stack.append(boolean_OR_NOT(operands))\n    \n    return stack[-1]\n\nresult = process_query(df['preprocessed_text'], 'amazon AND advertised') \nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.135686Z","iopub.execute_input":"2024-02-09T17:45:13.136688Z","iopub.status.idle":"2024-02-09T17:45:13.204626Z","shell.execute_reply.started":"2024-02-09T17:45:13.136648Z","shell.execute_reply":"2024-02-09T17:45:13.203220Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"{0, 258, 740, 807, 713, 463, 306, 597, 856, 698, 251, 734}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(list(df['preprocessed_text'][:1]))\nprint()\nprint(list(df['preprocessed_text'][1:2]))","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.205992Z","iopub.execute_input":"2024-02-09T17:45:13.207113Z","iopub.status.idle":"2024-02-09T17:45:13.213869Z","shell.execute_reply.started":"2024-02-09T17:45:13.207077Z","shell.execute_reply":"2024-02-09T17:45:13.212753Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"['amazon advertised pictures item fender fte pre amp description says fishman isys iii type googled found two version pre amp type bucket ce series one got today fishman type pre amp much better fte box fell inlove color design mine tone sunburst action right tone sound awesome hooked vox amp say thing really rocks course get better sounding acoustic electric guitar much expensive price family man two kids gonna spend thousand bucks hobby amplified love tone baby appearance sound quality course price giving five stars uploaded pictures fishman pre amp version bucket series']\n\n['really like simplicity bridge adjusts easy string height length comes mounting screws even small allen wrench make height adjustments provided mounting screws phillips head screws one thing missed ordered fact bridge string thru guitar body placement strings dead end stop flair plate included picture may help make others aware string thru applications biggie tho use another project']\n","output_type":"stream"}]},{"cell_type":"code","source":"def process_queries(corpus, input_data):\n    # Create inverted index and term frequency dictionary\n    inverted_index = {}\n    doc_term_freq = {}\n    for doc_id, document in enumerate(corpus):\n        terms = document.split()\n        doc_term_freq[doc_id] = {}\n        for term in terms:\n            if term not in inverted_index:\n                inverted_index[term] = set()\n            inverted_index[term].add(doc_id)\n            doc_term_freq[doc_id][term] = doc_term_freq[doc_id].get(term, 0) + 1\n    \n    # Define boolean operations\n    def boolean_AND(operands):\n        result = set(range(len(corpus)))\n        for operand in operands:\n            result = result.intersection(operand)\n        return result\n\n    def boolean_OR(operands):\n        result = set()\n        for operand in operands:\n            result = result.union(operand)\n        return result\n\n    def boolean_AND_NOT(operands):\n        result = operands[0]\n        for operand in operands[1:]:\n            result = result.difference(operand)\n        return result\n\n    def boolean_OR_NOT(operands):\n        result = operands[0]\n        for operand in operands[1:]:\n            result = result.union(result.difference(operand))\n        return result\n\n    # Process queries\n    results = []\n    num_queries = input_data[\"num_queries\"]\n    queries = input_data[\"queries\"]\n    operations = input_data[\"boolean_functions\"]\n    for i in range(num_queries):\n        query = queries[i]\n        operation = operations[i]\n        # Process query\n        stack = []\n        query_terms = query.split()\n        for term in query_terms:\n            if term not in operations:\n                stack.append(inverted_index.get(term, set()))\n            elif term == 'NOT':\n                if stack:\n                    operand = stack.pop()\n                    result = set(range(len(corpus))).difference(operand)\n                    stack.append(result)\n            else:\n                operands = []\n                while stack and stack[-1] not in operations:\n                    operands.append(stack.pop())\n                if operands:\n                    if term == 'AND':\n                        stack.append(boolean_AND(operands))\n                    elif term == 'OR':\n                        stack.append(boolean_OR(operands))\n                    elif term == 'AND_NOT':\n                        stack.append(boolean_AND_NOT(operands))\n                    elif term == 'OR_NOT':\n                        stack.append(boolean_OR_NOT(operands))\n        results.append(stack[-1])\n\n    return results\n\n# Sample input\nquery1 = \"amazon advertised pictures\"\nquery2 = \"really like simplicity\"\n\ninput_data = {\"num_queries\": 2,\n    \"queries\": [query1, query2],\n    \"boolean_functions\": [['OR', 'AND NOT'], ['AND', 'OR NOT']]} \n\n# Process queries\nresults = process_queries(df['preprocessed_text'], input_data)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.215708Z","iopub.execute_input":"2024-02-09T17:45:13.216514Z","iopub.status.idle":"2024-02-09T17:45:13.279537Z","shell.execute_reply.started":"2024-02-09T17:45:13.216481Z","shell.execute_reply":"2024-02-09T17:45:13.278275Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Output results\nfor i, query in enumerate(input_data[\"queries\"]):\n    query_terms = query.split()\n    boolean_function = input_data[\"boolean_functions\"][i]\n    \n    # Ensure that the lengths match\n    min_length = min(len(query_terms), len(boolean_function))\n    \n    # Construct the query with boolean operators applied to each term\n    query_with_boolean = ' '.join([f\"{query_terms[j]} {boolean_function[j]}\" for j in range(min_length)])\n    \n    # Append the remaining terms of the query if any\n    if min_length < len(query_terms):\n        query_with_boolean += f\" {' '.join(query_terms[min_length:])}\"\n    \n    print(f\"Query {i + 1}: {query_with_boolean}\")\n    print(\"Number of documents retrieved for query\",i+1 ,\": {}\".format(len(results[i])))\n    #print(\"Row indices of the documents rtrieved: {}\".format(', '.join([str(doc_id) for doc_id in results[i]])))\n    print(\"Name of the documents retrieved for query\",i+1 ,\":\", end=\"\")\n    print(', '.join([re.search(r'/([^/]+)$', df.loc[doc_id, 'path']).group(1) for doc_id in results[i]]))\n    print()\n    \n    print()\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.281031Z","iopub.execute_input":"2024-02-09T17:45:13.281430Z","iopub.status.idle":"2024-02-09T17:45:13.294766Z","shell.execute_reply.started":"2024-02-09T17:45:13.281399Z","shell.execute_reply":"2024-02-09T17:45:13.293865Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Query 1: amazon OR advertised AND NOT pictures\nNumber of documents retrieved for query 1 : 24\nName of the documents retrieved for query 1 :file314.txt, file24.txt, file125.txt, file634.txt, file511.txt, file982.txt, file859.txt, file22.txt, file28.txt, file108.txt, file420.txt, file226.txt, file518.txt, file422.txt, file542.txt, file640.txt, file68.txt, file118.txt, file352.txt, file918.txt, file232.txt, file289.txt, file172.txt, file208.txt\n\n\nQuery 2: really AND like OR NOT simplicity\nNumber of documents retrieved for query 2 : 3\nName of the documents retrieved for query 2 :file998.txt, file819.txt, file251.txt\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q3. Positional Index and Phrase Queries","metadata":{}},{"cell_type":"markdown","source":"## 1. Create a positional index (from scratch; No library allowed) of the dataset obtained from Q1","metadata":{}},{"cell_type":"code","source":"# Function for creating positional index\ndef create_positional_index(data):\n    positional_index = {}\n    for index, text in enumerate(data):\n        words = text.split()\n        for position, word in enumerate(words):\n            if word in positional_index:\n                positional_index[word].append((index, position))\n            else:\n                positional_index[word] = [(index, position)]\n    return positional_index\n\n# Assuming data is the list of preprocessed texts obtained from Q1\npositional_index = create_positional_index(df['preprocessed_text'])\ncount = 0\n\nfor word, positions in positional_index.items():\n    count = count + 1\n    print(word, ':', positions)\n    if count == 5:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.296132Z","iopub.execute_input":"2024-02-09T17:45:13.297310Z","iopub.status.idle":"2024-02-09T17:45:13.447782Z","shell.execute_reply.started":"2024-02-09T17:45:13.297276Z","shell.execute_reply":"2024-02-09T17:45:13.446209Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"amazon : [(0, 0), (7, 25), (9, 9), (64, 32), (64, 98), (70, 7), (83, 26), (130, 3), (144, 6), (151, 10), (157, 51), (170, 0), (170, 28), (191, 27), (191, 48), (270, 35), (295, 83), (300, 26), (300, 37), (326, 5), (326, 20), (327, 29), (330, 74), (346, 8), (350, 44), (354, 8), (371, 26), (371, 45), (386, 43), (401, 28), (401, 46), (418, 28), (418, 47), (420, 19), (445, 28), (445, 46), (445, 119), (450, 27), (450, 46), (450, 80), (456, 7), (466, 37), (476, 19), (530, 36), (534, 42), (538, 63), (549, 24), (555, 21), (557, 9), (595, 3), (614, 54), (616, 27), (616, 48), (654, 6), (655, 14), (693, 7), (745, 28), (745, 46), (764, 31), (764, 52), (769, 28), (769, 48), (773, 69), (807, 20), (838, 4), (838, 13), (878, 54), (880, 45), (880, 57), (881, 3), (881, 45), (920, 82), (952, 28), (952, 47), (954, 34), (957, 53)]\nadvertised : [(0, 1), (251, 99), (258, 8), (306, 8), (463, 3), (597, 1), (698, 1), (713, 0), (734, 1), (740, 19), (807, 22), (856, 2)]\npictures : [(0, 2), (0, 86), (8, 21), (43, 29), (91, 21), (114, 53), (128, 54), (223, 41), (241, 10), (275, 15), (299, 79), (304, 26), (351, 30), (367, 21), (452, 30), (458, 8), (535, 11), (579, 8), (581, 57), (628, 34), (683, 52), (709, 69), (772, 4), (813, 84), (990, 4)]\nitem : [(0, 3), (38, 22), (49, 3), (64, 34), (144, 9), (199, 44), (327, 37), (327, 41), (327, 45), (327, 50), (389, 27), (434, 32), (437, 129), (530, 28), (531, 1), (533, 4), (548, 1), (580, 11), (591, 3), (591, 33), (655, 1), (826, 20), (826, 46), (831, 8), (842, 1), (842, 12), (842, 44), (867, 54), (878, 28), (907, 11), (907, 52), (992, 32)]\nfender : [(0, 4), (19, 42), (35, 5), (35, 18), (35, 34), (35, 52), (42, 2), (42, 8), (42, 52), (53, 1), (53, 23), (60, 3), (60, 8), (84, 0), (84, 18), (85, 19), (85, 32), (85, 43), (90, 5), (90, 72), (100, 20), (184, 11), (184, 33), (207, 1), (207, 5), (209, 54), (213, 29), (218, 2), (218, 13), (218, 20), (219, 48), (219, 56), (230, 6), (230, 12), (249, 15), (253, 27), (278, 40), (281, 3), (315, 5), (315, 8), (317, 0), (361, 10), (366, 3), (376, 27), (383, 19), (436, 42), (448, 64), (464, 1), (466, 58), (467, 39), (467, 45), (500, 5), (503, 4), (557, 3), (603, 6), (604, 8), (607, 4), (613, 28), (613, 82), (614, 13), (625, 3), (633, 44), (652, 5), (654, 52), (656, 24), (661, 2), (668, 38), (677, 6), (684, 14), (695, 31), (725, 19), (763, 32), (766, 3), (779, 5), (818, 76), (818, 83), (819, 1), (831, 18), (891, 10), (891, 11), (915, 10), (927, 18), (927, 21), (945, 2), (987, 5)]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(list(df['preprocessed_text'][:1]))\nprint()\nprint(list(df['preprocessed_text'][1:2]))\nprint()\nprint(list(df['preprocessed_text'][2:3]))","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.449310Z","iopub.execute_input":"2024-02-09T17:45:13.449799Z","iopub.status.idle":"2024-02-09T17:45:13.458418Z","shell.execute_reply.started":"2024-02-09T17:45:13.449756Z","shell.execute_reply":"2024-02-09T17:45:13.457034Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"['amazon advertised pictures item fender fte pre amp description says fishman isys iii type googled found two version pre amp type bucket ce series one got today fishman type pre amp much better fte box fell inlove color design mine tone sunburst action right tone sound awesome hooked vox amp say thing really rocks course get better sounding acoustic electric guitar much expensive price family man two kids gonna spend thousand bucks hobby amplified love tone baby appearance sound quality course price giving five stars uploaded pictures fishman pre amp version bucket series']\n\n['really like simplicity bridge adjusts easy string height length comes mounting screws even small allen wrench make height adjustments provided mounting screws phillips head screws one thing missed ordered fact bridge string thru guitar body placement strings dead end stop flair plate included picture may help make others aware string thru applications biggie tho use another project']\n\n['truthfully idea even ukulele capos use guitars banjo certainly times might want shift key without transposing take advantage open strings regardless well made lightweight capo weight particular importance dealing instrument already small lightweight hernally played without strap capo make soprano uke neck heavy soft rubber surface moveable jaw good job pressing synthetic strings fret without stretching tune useful tool ukuleleists']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Use Pythonâ€™s pickle module to save and load the positional index.","metadata":{}},{"cell_type":"code","source":"import pickle\n\nwith open('positional_index.pkl', 'wb') as file:\n    pickle.dump(positional_index, file)\n\nwith open('positional_index.pkl', 'rb') as file:\n    loaded_positional_index = pickle.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.460377Z","iopub.execute_input":"2024-02-09T17:45:13.460827Z","iopub.status.idle":"2024-02-09T17:45:13.503853Z","shell.execute_reply.started":"2024-02-09T17:45:13.460792Z","shell.execute_reply":"2024-02-09T17:45:13.502405Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Function for retrieving documents based on query using positional index\ndef retrieve_documents(positional_index, query):\n    if isinstance(query, str):\n        query_terms = query.split()\n    else:\n        query_terms = query\n    document_candidates = {}\n    \n    # Initialize document_candidates with all documents\n    for term in query_terms:\n        if term in positional_index:\n            for document_position in positional_index[term]:\n                document_id = document_position[0]\n                if document_id in document_candidates:\n                    document_candidates[document_id].append(document_position)\n                else:\n                    document_candidates[document_id] = [document_position]\n\n    # Filtering documents where all terms appear in correct order\n    retrieved_documents = []\n    for document_id, positions in document_candidates.items():\n        positions.sort()\n        match = True\n        for i in range(len(positions) - 1):\n            if positions[i+1][0] != positions[i][0] or positions[i+1][1] - positions[i][1] != 1:\n                match = False\n                break\n        if match:\n            retrieved_documents.append(document_id)\n    \n    return retrieved_documents\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.505073Z","iopub.execute_input":"2024-02-09T17:45:13.505455Z","iopub.status.idle":"2024-02-09T17:45:13.517255Z","shell.execute_reply.started":"2024-02-09T17:45:13.505422Z","shell.execute_reply":"2024-02-09T17:45:13.515576Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"queries = [3, \"amazon\", \"simplicity\", \"fender\"]\n\n# Extract the number of queries\nnum_queries = queries[0]\n\n# Process queries and retrieve documents\nfor query_number, query_text in enumerate(queries[1:], start=1):\n    print(f\"Query {query_number}: {query_text}\")\n    retrieved_docs = retrieve_documents(loaded_positional_index, query_text)\n    print(f\"Number of documents retrieved for Query {query_number} using positional index: {len(retrieved_docs)}\")\n    print(\"Names of documents retrieved for Query\",query_number, 'using positional index:', end=\" \")\n    for doc_id in retrieved_docs:\n        row_index = df.index[doc_id]\n        column_value = df.at[row_index, \"path\"]\n        column_value = re.search(r'/([^/]+)$', column_value).group(1)\n        print(column_value, end=\", \")\n    print() ","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.519046Z","iopub.execute_input":"2024-02-09T17:45:13.519509Z","iopub.status.idle":"2024-02-09T17:45:13.537699Z","shell.execute_reply.started":"2024-02-09T17:45:13.519474Z","shell.execute_reply":"2024-02-09T17:45:13.536024Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Query 1: amazon\nNumber of documents retrieved for Query 1 using positional index: 38\nNames of documents retrieved for Query 1 using positional index: file314.txt, file65.txt, file988.txt, file343.txt, file669.txt, file965.txt, file205.txt, file241.txt, file412.txt, file656.txt, file33.txt, file363.txt, file835.txt, file305.txt, file96.txt, file908.txt, file916.txt, file866.txt, file571.txt, file896.txt, file324.txt, file706.txt, file114.txt, file637.txt, file905.txt, file311.txt, file145.txt, file583.txt, file915.txt, file514.txt, file408.txt, file176.txt, file347.txt, file593.txt, file216.txt, file684.txt, file813.txt, file133.txt, \nQuery 2: simplicity\nNumber of documents retrieved for Query 2 using positional index: 3\nNames of documents retrieved for Query 2 using positional index: file998.txt, file819.txt, file251.txt, \nQuery 3: fender\nNumber of documents retrieved for Query 3 using positional index: 45\nNames of documents retrieved for Query 3 using positional index: file314.txt, file11.txt, file741.txt, file301.txt, file440.txt, file186.txt, file847.txt, file246.txt, file67.txt, file131.txt, file562.txt, file797.txt, file659.txt, file879.txt, file733.txt, file166.txt, file759.txt, file896.txt, file277.txt, file380.txt, file145.txt, file604.txt, file995.txt, file313.txt, file915.txt, file278.txt, file627.txt, file432.txt, file514.txt, file245.txt, file197.txt, file73.txt, file256.txt, file883.txt, file183.txt, file617.txt, file725.txt, file980.txt, file843.txt, file674.txt, file484.txt, file680.txt, file803.txt, file95.txt, file822.txt, \n","output_type":"stream"}]},{"cell_type":"code","source":"# Support for Phrase Queries\nimport re\ndef retrieve_documents(positional_index, query):\n    if isinstance(query, str):\n        query_terms = query.split()\n    else:\n        query_terms = query\n    document_candidates = {}\n    \n    # Initialize document_candidates with all documents\n    for term in query_terms:\n        if term in positional_index:\n            for document_position in positional_index[term]:\n                document_id = document_position[0]\n                if document_id in document_candidates:\n                    document_candidates[document_id].append(document_position)\n                else:\n                    document_candidates[document_id] = [document_position]\n\n    # Filter documents where at least one term appears and all terms appear\n    retrieved_documents = []\n    for document_id, positions in document_candidates.items():\n        if any(doc_pos[0] == document_id for doc_pos in positions):\n            retrieved_documents.append(document_id)\n        elif all(term in [pos[0] for pos in positions] for term in query_terms):\n            retrieved_documents.append(document_id)\n    \n    return list(set(retrieved_documents))\nqueries = [\n    \"simplicity\", \n    \"fender amazon\"]\n\n# Process queries and retrieve documents\nfor i, query in enumerate(queries, 1):\n    print(i, query)\n    retrieved_docs = retrieve_documents(loaded_positional_index, query)\n    print(f\"Number of documents retrieved for query {i} using positional index: {len(retrieved_docs)}\")\n    print(f\"Number of documents retrieved for query {i} using positional index:\")\n    for doc_id in retrieved_docs:\n        row_index = df.index[doc_id]\n        column_value = df.at[row_index, \"path\"]\n        column_value = re.search(r'/([^/]+)$', column_value).group(1)\n        print(column_value, end=\", \")\n    print() ","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:45:13.539740Z","iopub.execute_input":"2024-02-09T17:45:13.540192Z","iopub.status.idle":"2024-02-09T17:45:13.559961Z","shell.execute_reply.started":"2024-02-09T17:45:13.540154Z","shell.execute_reply":"2024-02-09T17:45:13.558747Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"1 simplicity\nNumber of documents retrieved for query 1 using positional index: 3\nNumber of documents retrieved for query 1 using positional index:\nfile998.txt, file819.txt, file251.txt, \n2 fender amazon\nNumber of documents retrieved for query 2 using positional index: 113\nNumber of documents retrieved for query 2 using positional index:\nfile314.txt, file65.txt, file988.txt, file706.txt, file11.txt, file114.txt, file637.txt, file558.txt, file905.txt, file439.txt, file311.txt, file145.txt, file652.txt, file346.txt, file742.txt, file343.txt, file669.txt, file46.txt, file174.txt, file583.txt, file302.txt, file604.txt, file995.txt, file313.txt, file741.txt, file907.txt, file915.txt, file102.txt, file278.txt, file627.txt, file965.txt, file432.txt, file514.txt, file408.txt, file245.txt, file205.txt, file197.txt, file241.txt, file73.txt, file412.txt, file256.txt, file460.txt, file883.txt, file176.txt, file183.txt, file150.txt, file913.txt, file623.txt, file301.txt, file440.txt, file617.txt, file638.txt, file910.txt, file455.txt, file488.txt, file186.txt, file725.txt, file334.txt, file847.txt, file980.txt, file390.txt, file347.txt, file843.txt, file656.txt, file246.txt, file67.txt, file33.txt, file593.txt, file737.txt, file609.txt, file674.txt, file14.txt, file131.txt, file484.txt, file999.txt, file363.txt, file372.txt, file835.txt, file305.txt, file96.txt, file908.txt, file562.txt, file797.txt, file216.txt, file388.txt, file701.txt, file663.txt, file659.txt, file680.txt, file879.txt, file916.txt, file569.txt, file803.txt, file684.txt, file559.txt, file254.txt, file866.txt, file95.txt, file733.txt, file105.txt, file813.txt, file748.txt, file133.txt, file166.txt, file672.txt, file571.txt, file759.txt, file896.txt, file691.txt, file822.txt, file324.txt, file277.txt, file380.txt, \n","output_type":"stream"}]}]}